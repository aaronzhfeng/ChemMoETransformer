# ---------------------------------------------------------------------------
#  Example configuration that enables MoE Feed‑Forward blocks
#  in encoder & decoder layers {2,4,6}.  Each MoE layer has 4 experts
#  using Expert‑Choice routing (router params set inside the model code).
# ---------------------------------------------------------------------------
model:
  n_layer: 6
  d_model: 512
  n_head: 8
  d_ff: 2048
  dropout: 0.1
  moe_layers: [2, 4, 6]     # 1‑based indexing for readability
  num_experts: 4            # homogeneous FFN experts per MoE layer

data:
  data_root: data/USPTO_480k/
  vocab_file: data/USPTO_480k/vocab_smiles.txt
  batch_size: 256
  num_workers: 8


training:
  max_epoch:   80
  max_lr:      2.0e-4
  min_lr:      2.0e-5
  weight_decay: 0.0
  grad_clip:   1.0
  seed:        42
  log_interval: 500
  device:      cuda:1
  aux_loss_weight: 0.02   # λ in  loss = CE + λ · aux
  debug_samples: 1000     # if --debug flag is used
  
output:
  ckpt_dir: checkpoints/
  ckpt_interval: 10
  log_file:   train_moe.log
